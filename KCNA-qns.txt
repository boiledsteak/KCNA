kubernetes & cloud native associate QUESTIONS

not KCNA exam dump but possible KCNA exam questions

1. what does CNCF stand for

2. what does linux foundation do?

3. what are the goals of cloud native architecture

4. what does immutable infrastructure mean? Benefits? Downisdes?

5. pros and cons of monolithic infrastructure?

6. vertical vs horizontal scaling

7. What is CloudEvents?

8. What is OCI and what are the components?

9. what are the CNCF project stages?

10. what are the metrics for SRE?

11. 




k8 2 parts
1. control plane: management of nodes
2. data plane: worker nodes


service mesh: a layer of multiple proxies that sit between server and client. used for monitoring amongst other things
-> the control plane and data plane is inside the service mesh
-> things outside service mesh is like nginx, cicd tools, prometheus
-> typically uses a sidecar to offload port mapping





docker containers share the same kernel as the host


components of the control plane
1. kube-apiserver
-> all components interact with this
-> this is where users access the cluster

2. etcd
-> a db that stores the state of the cluster

3. kube-scheduler
-> assigns worker node to new scheduled workload

4. kube-controller-manager
-> several control systems that manage the state of the cluster. ie set the number of apps available at all times



components of the worker nodes/ data plane
1. container runtime
-> runs the containers on the nodes. containerd

2. kubelet [ESSENTIAL]
-> agent on every node that talks to the kube-apiserver
-> ensures the node can be managed by the cluster

3. kube-proxy [ESSENTIAL]
-> when a request reaches cluster IP, kube-proxy forwards to the right pod
-> runs on each node
-> allows pods to talk to one another




with the above architecture, apps running on worker nodes can still run without control plane


can create virtual clusters using namespace as well. good for very large clusters where you need finer control of resources and permissions by using ROLE BASED ACCESS CONTROL (RBAC)


4 kinds of objects within RBAC
	1. role
		-> specifies permissions for a namespace

			EXMAPLE (only allows pods to perform get, list, and watch within my-namespace)


			apiVersion: rbac.authorization.k8s.io/v1
			kind: Role
			metadata:
			  namespace: my-namespace
			  name: my-role
			rules:
			- apiGroups: [""]
			  resources: ["pods"]
			  verbs: ["get", "list", "watch"]



	2. clusterRole
		-> specifies permissions within the cluster, so across multiple namespaces


	3. RoleBinding
		-> more explicit specification of permissions from a pre-defined role object
		-> can apply to user, group, or service account within a specific namespace


	4. ClusterRoleBinding
		-> more explicit specification of permissions from a pre-defined ClusterRole object
		-> can apply to user, group, or service account across several namespaces



ingress: a collection of rules that allow inbound connections into the cluster



kubernetes API process:

all API requests go thorugh these steps

1. Authentication: verify identity with cert or other identity management system

2. Authorisation: decide what the requester can do with RBAC

3. Admission Control: validates request. check if request is dangerous etc..




by default, all pods within a cluster can talk to one another.
to control this behaviour, need to configure Network Policy
requires a controller to implement the network policy

such controller for example: cillium

ingress objects work at application layer while network policy works at network layer

ingress directs traffic thats going towards a certain domain, to the right service (pods)

a service is a logical group of pods


scheduling: selecting the right node for the workload

kubernetes is declarative. need to declare hardware requirements for a workload, then the kube-scheduler will find a node that fits the requirements. if several nodes fit, the node with least number of pods is picked

if cannot find a node to fit the requirements, the kube-scheduler will keep retrying until a node that fits, exists 


can config certain pods to be scheduled to certain nodes
these are the methods

NodeSelector
- add NodeSelector field in pod YAMML and specify which node
- several pods can be tied to single NodeSelector label

Affinity
- can indicate if a selected node is soft rule or hard

NodeName
- schedules specific pod to specific node



taints and constraints
- NodeSelector asks scheduler to schedule pods to certain nodes based on some specifications
- tains is the opposite. AVOID scheduling certain pods to certain nodes based on some specifications

A taint consists of a key, value, and effect. 
	key=value:effect
ie
	kubectl taint node worker region=useast2:NoSchedule






pod vs node:

- pod is smallest unit in kubernetes infrastructure
- node is the VM or machine that holds the entire cluster
- cluster has many nodes
- nodes have many pods
- pods have container



k8 objects

- workload-oriented objects
  = handle workloads

workload-oriented controller objects
-- ReplicaSet
  = ensures a specified number of pods are running at all times
  - used during scaling

-- deployment
  = manages multiple ReplicaSets
  = the big object with 1x YAML file. ie gitlab app YAML is considered a deployment

-- StatefulSet
  = deployments with persistent data like database
  = usually requires headless service object (headless service returns a static ip of one pod instead of clusterIP)
  = static ip addr

-- DaemonSet
	= ensures all nodes run a copy of the pods
	= automatically adds new pods to new nodes

-- jobs
	= creates pods to perform a task and terminates after
	= use this key:
		ttlSecondsAfterFinished

-- cronJob
	= creates jobs, but allocates a time requirement ie. every morning




- infrastructure-oriented objects (service objects)
  = handle config

-- ClusterIP service (sometimes known as service IP)
	= gives virtual IP to a set of pods

-- NodePort
	= opens ports on all nodes in a cluster

-- LoadBalancer
	= yknow

-- ExternalName
	= create alias for public (external) websites

-- Headless Service
	= when no need load balancing, use this




--> all delcared in YAML


compulsary YAML fields
1. apiVersion: each object's version
2. kind: the kind of object to create
3. metadata: uniquely identifying data like a name. can create namespace for objects with same name
4. spec: desired state of the object like how much storage, what ports to use



pods dont need NAT talk to one another







to access k8 node, need to access the k8 api. this can be done using kubectl



helm: package manager for kubernetes. updates the objects in your node




all containers within a pod share the same IP address

initContainers: if container is defined as initContainers in YAML, then it will start first before main container starts. a sidecar container


key settings to set for containers in a pod
1. resources: cpu,  storage etc
2. livenessProbe: periodically check if app is alive. restart container if no response
3. securityContext: user and group settings



pod lifecycle
1. pending: 
- pod accepted by cluster but container not running yet. 
- pod could be waiting to be scheduled. 
- pod only run when got workload scheduled. 
- pod could also be downloading/ setting up the container / image

2. running:
- pod up and container running

3. succeeded:
- all containers in pod running

4. failed:
- all containers in pod stopped
- at least one container exit with failed status

5. unknown:
- state of pod could not be determined
- could be issue communicating with pod




mounted volumes talk to the pod, not the container
mounted volumes can share data between containers, within a pod



PersistentVolume & PersistentVolumeClaims (PVC)
- PVC is created with how much storage and location of storage, declared

- When a PVC is created, Kubernetes looks for a matching PV. 
If a suitable PV is found, it binds the PVC to the PV. 
If not, the PVC will remain unbound until a matching PV becomes available

- Once bound, the PVC can be used by a pod to mount the storage defined by the PV

- When the PVC is deleted, the PV becomes “released” but retains the data. 
The actual data cleanup depends on the reclaim policy (Retain, Recycle, Delete) defined on the PV



ConfigMap
- maps config files (YAML) of all nodes in the cluster
- maps ENV variables too




Autoscalers

1. Horizontal pod autoscaler (HPA)
	- watches ReplicaSets and scales accordingly

2. Cluster autoscaler
	- works with HPA to scale clusters

3. Vertical pod autoscaler (VPA)
	- enables vertical scaling for nodes


metrics-server
- enables horizontal scaling
- can be replaced with Prometheus Adapter for Kubernetes Metrics APIs



kebernetes-based event driven autoscaler (KEDA)
- auto scale based on events such as database query instead of metrics






K3: k8 distro built for IoT and edge computing



Open Policy Agent(OPA): enforce policies



custom resource definition (CRD): api extension to define custom resources


kata: docker but better security


kubernetes API serverdefault authorisation method is ALLOW ALL




___________________________

gitops & ArgoCD
___________________________


GitOps uses pull based approach

agent watches the git repo for any changes, particularly to IAC. 
If there are changes then infrastructure is also updated accordingly



___________________________

observability

___________________________

observability is not the same as monitoring

observability is the bigger umbrella. There are more categories apart from monitoring

what's more important than monitoring, is ensuring nodes in a system are still managed despite being under load or in error state

goal of observability: analysis of system from collected data

analyse the behavior of software and adapt it constantly based on the outcome



3 types of data containers provide for observability
	1. logs
		- error messages, warnings, text based

	2. metrics
		- quantitative data ie. number of requests

	3. traces
		- how long a request takes, status of request
		- ie. jaeger


3 ways to send logging data to centralised repo
	1. node-level logging
		= log shipping tool per node

	2. logging via sidecar container
		= sidecar container per container

	3. application-level logging
		= app itself sends log info to central repo


grafana loki: ship and store logs


Prometheus 4 core metrics
	a) counter
		= simple number that increases. error count etc

	b) gauge
		= values that go up and down. memory size etc

	c) histogram
		= yknow

	d) summary
		= histogram + total count of observations


AlertManager: tool that works with prometheus to send alerts when certain metrics' threshold are hit


Jaeger: tracing 


linkerd: manages traffic flows between services, enforces access policies, and aggregates telemetry data, all without requiring changes to application code


k8 objects provides a "record of intent"


6 layers of CNCF landscape
- provisioning
- runtime
- orchestration & management
- app definition and development
- observability & analysis
- platform


kubectl list all api groups:
	kubectl api-versions


container network interface (CSI):
	a framework that defines how plugins and k8  should manage container networking, in a standardised way

Container Runtime Interface (CRI):
	allows multiple runtimes ie. docker and containerd to work together without re-compiling



label selector: the app=nginx part 
	kubectl get pods -l app=nginx

field selector the --field-selector=status.phase=Running part
	kubectl get pods --field-selector=status.phase=Running



ReplicationControllers
-> old tech, replaced by ReplicaSet
-> does not support set-based label selectors

EXAMPLE YAML:

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: example-replicaset
spec:
  replicas: 3
  selector:
	matchLabels:
	  app: nginx
	matchExpressions:
	  - key: environment
		operator: In
		values: ["production", "staging"]



containerd: designed to be more simple than docker


canary deployment: deploy the new app to a small group of users while still running the old app. when okay with small group then fully release new app


blue/green deployment: production and dev versions. can switch to either one with zero downtime


Pod Security Admission (PSA): [aka admission controller]
	replaced PodSecurityPolicies key
	-> apply security profiles to namespaces

	EXAMPLE

	apiVersion: v1
		kind: Namespace
		metadata:
		  name: my-namespace
		  labels:
			pod-security.kubernetes.io/enforce: baseline



	3 security profiles u can pick
		1. privileged: least security
		2. baselined: minimal security
		3. restricted: most security


in a hub and spoke architecture, k8's hub is the API server 


/metrics
	-> standard endpoint used by k8 objects to expose their metrics


Cloud Controller Manager: link your cluster to cloud provider


CoreDNS: the default DNS server for K8


knative: serverless platform for K8 focused on dev

fission: serverless framework for k8


metadata in k8 objects .yaml
-> annotations
-> labels

	EXAMPLE

	apiVersion: v1
	kind: Pod
	metadata:
	  name: example-pod
->	  labels:
	    app: myapp
	    environment: production
->	  annotations:
	    description: "This is an example pod"
	    version: "1.0.0"
	    maintainer: "devops@example.com"
	spec:
	  containers:
	  - name: myapp-container
	    image: nginx:latest
	    ports:
	    - containerPort: 80





thrashing: when horizontal scaling occurs too rapidly due to highly volatile metrics. causes degrade in performance


attribute-based access control: finer control of access. can specify ip addr instead of just role


imagePullPolicy: key that specifies when image will be pulled for container

	EXMAPLE

	apiVersion: v1
	kind: Pod
	metadata:
	  name: example-pod
	spec:
	  containers:
	  - name: myapp-container
	    image: myapp:latest
	    imagePullPolicy: Always



NetworkPolicy: set whether ingress or egress

default k8 update: rolling updates



helm install vs helm upgrade --install
-> install means install from whatever chart that exists
-> upgrade --install means upgrade the chart first then install



cgroups (control groups)


required fields for all k8 .yaml
- apiVersion
- kind
- metadata
- spec


worker nodes run workloads


All containers in a pod share a single IP address


kubeadm: used for cluster bootstrapping



___________________________

K8 security

___________________________

- harden hardware, software, firmware
- harden config options (YAML)
- harden network
- security in depth
- firewalls
- pod to pod encryption
- minimise base image
- ensure immutability
- SAST, DAST
- SELinux
- NIDs, SIEM, behavioural analytics









____________________

k8 commands

____________________



-> list available objects in node (see all items declared by yaml)
kubectl api-resources


-> explain something about k8
kubectl explain pod


-> create k8 object from YAML
kubectl create -f <your-file>.yaml





